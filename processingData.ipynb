{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W8a7NtdHmrm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "70ccdae1-870e-4a1e-9664-71528c8540ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datacompy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-219fd24772ad>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# !pip install datacompy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#  download it in the file where you are importing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatacompy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datacompy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from scipy.stats import lognorm\n",
        "import statsmodels.api as sm\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import lognorm\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "#  1\n",
        "# !pip install datacompy\n",
        "#  download it in the file where you are importing\n",
        "import datacompy\n",
        "import random\n",
        "import scipy.stats\n",
        "import math\n",
        "from scipy import stats\n",
        "from scipy.stats import ks_2samp\n",
        "from keras.layers import Input,Dense\n",
        "from keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#  2\n",
        "# !pip install pyclustering\n",
        "# download it in the file where you are importing\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "mUgRNU7daQR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=pd.read_csv(input('Enter the path of your dataset'))\n",
        "# /content/train (5).csv\n"
      ],
      "metadata": {
        "id": "tvbLEE7pHtWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('No. of observations:',len(t))"
      ],
      "metadata": {
        "id": "n4QMmIHXHwOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(train):\n",
        "  train=train.reset_index()\n",
        "  train.drop('index',axis=1,inplace=True)\n",
        "  print(train.head(),'\\n')\n",
        "  print(train.info(),'\\n')\n",
        "  print('Columns are:',train.columns,'\\n')\n",
        "  print(train.describe(),'\\n')\n",
        "\n",
        "  print('enter the index column name: ')\n",
        "  index_col=input()\n",
        "  print('Enter categorical columns: ')\n",
        "  categories=list(map(str,input().split()))\n",
        "  numeric=[]\n",
        "  train1=train.copy()\n",
        "  for i in train1.columns:\n",
        "    if i not in categories and i!=index_col:\n",
        "      numeric.append(i)\n",
        "\n",
        "  \"\"\"\n",
        "    CHECK FOR MISSING VALUES.\n",
        "  IMPUTE DATA IF NEEDED\"\"\"\n",
        "  def check_and_fill(train):\n",
        "    print('Enter the missing threshold and the correlation threshold: ')\n",
        "    miss_perc,corr_threshold=map(float,input().split())\n",
        "    for col in train.columns:\n",
        "      if(train[col].isnull().sum()==0):\n",
        "        pass\n",
        "      else:\n",
        "        countMiss=train[col].isnull().sum()\n",
        "        numOfObs=len(train)\n",
        "        if(countMiss/numOfObs*100)>miss_perc and -(corr_threshold)<train[col].corr(train['target'])<corr_threshold:\n",
        "          train=train.drop(col,axis=1)\n",
        "        elif countMiss>0:\n",
        "          imputeVal=input('Enter 1 for KNNImputer, 2 for Simple Imputer: ')\n",
        "          if imputeVal=='1':\n",
        "            impute=KNNImputer(n_neighbors=2)\n",
        "          elif imputeVal=='2':\n",
        "            impute=SimpleImputer(strategy='most_frequent')\n",
        "          train=impute.fit_transform(train)\n",
        "    return train\n",
        "\n",
        "  train1=pd.DataFrame(check_and_fill(train1))\n",
        "  #numeric and categories- lists which store\n",
        "  # train1.columns\n",
        "  if index_col in train1:\n",
        "    train1.drop(index_col,axis=1,inplace=True) #dropping id column\n",
        "\n",
        "  #check for duplicates,remove them if needed\n",
        "  train1=train1.drop_duplicates()\n",
        "\n",
        "  #SHOWING DISTRIBUTION BALANCE OF CATEGORICAL DATA\n",
        "  def plot_categorical_balance(train,categories):\n",
        "    train=train[categories]\n",
        "    for col in categories:\n",
        "      sns.countplot(x=col,data=train)\n",
        "  plot_categorical_balance(train1,categories)\n",
        "\n",
        "  #Scaling the data\n",
        "  train2=train1.copy()\n",
        "  train2=train2.drop(categories,axis=1)\n",
        "\n",
        "  def scaling(train,scaler_type):\n",
        "    if scaler_type==1:\n",
        "      scaler=StandardScaler()\n",
        "    elif scaler_type==2:\n",
        "      scaler=RobustScaler()\n",
        "    elif scaler_type==3:\n",
        "      scaler=MinMaxScaler()\n",
        "    scaled_train=scaler.fit_transform(train)\n",
        "    return scaled_train\n",
        "\n",
        "  def get_scaler_number():\n",
        "    while True:\n",
        "      try:\n",
        "        scaler_type=int(input('enter 1 for Standard Scaler,2 for Robust Scaler, 3 for Min Max Scaler: '))\n",
        "        l=[1,2,3]\n",
        "        if scaler_type in l:\n",
        "          return scaler_type\n",
        "        else:\n",
        "          print(\"Invalid input. Please enter from the given list: \",l)\n",
        "      except ValueError:\n",
        "        print(\"Invalid input. Please enter correctly. \")\n",
        "\n",
        "  scaler_no=get_scaler_number()\n",
        "\n",
        "  #train3 will store the numeric scaled data\n",
        "  train3=pd.DataFrame(scaling(train2,scaler_no))\n",
        "\n",
        "  #train2 will store the scaled data and the categorical data\n",
        "  train2=pd.concat([train1[categories],train2],axis=1)\n",
        "\n",
        "  #Applying one hot encoding to the categorical columns except the target column\n",
        "  def one_hot_encoding(train,categories):\n",
        "    new_train=pd.DataFrame()\n",
        "    for col in categories:\n",
        "      if col!='target':\n",
        "        one_hot_data=pd.get_dummies(col,prefix=col,drop_first=True)\n",
        "        train.drop(col,axis=1,inplace=True)\n",
        "        new_train=pd.concat([new_train,one_hot_data],axis=1)\n",
        "      if col=='target':\n",
        "        new_train=pd.concat([new_train,train['target']],axis=1)\n",
        "    return new_train\n",
        "\n",
        "  #Stores the one hot encoded data only\n",
        "  train4=one_hot_encoding(train2,categories)\n",
        "\n",
        "  #Combining the one hot encoded data and the scaled data\n",
        "  train2=pd.concat([train3,train4],axis=1)\n",
        "  print('DATA after one-hot encoding: ',train2)\n",
        "\n",
        "  ytrain=train1['target'] # or get it from train2\n",
        "  #keep target column in one variable, train1 to get target values.\n",
        "  print(train2.info())\n",
        "  print(train2.columns)\n",
        "\n",
        "  #Univariate analysis\n",
        "  #Plotting Q-Q plots to check for normalization\n",
        "  #Check the skewness of data columns\n",
        "  import numpy as np\n",
        "  from scipy.stats import lognorm\n",
        "  import statsmodels.api as sm\n",
        "  np.random.seed(1)\n",
        "  # from scipy.stats import shapiro\n",
        "  import random\n",
        "  def plot_histograms(train,numeric):\n",
        "    feature=(np.random.choice(numeric))\n",
        "    plt.figure(figsize=(20,10))\n",
        "    sns.distplot(train[feature])\n",
        "    skew_val=train1.skew(axis=1)\n",
        "    print(f\"Skewness of {feature}: {skew_val}\")\n",
        "    #If plot is not along 45 degrees,dataset is not normally distributed, see visually\n",
        "    sm.qqplot(train1,line='45')\n",
        "    plt.show()\n",
        "  plot_histograms(train2,train3.columns)\n",
        "\n",
        "  print('Scatterplot')\n",
        "  def plot_scatter(train3):\n",
        "    a=np.random.choice(train1.columns)\n",
        "    b=np.random.choice(train1.columns)\n",
        "    print(a,b)\n",
        "    plt.scatter(x=train1[a],y=train1[b],cmap='viridis')\n",
        "    plt.xlabel(a)\n",
        "    plt.ylabel(b)\n",
        "  plot_scatter(train3)\n",
        "\n",
        "  #Normalization - Transformation techniques\n",
        "  def to_normalize(train):\n",
        "    # train=train[numeric]\n",
        "    func=int(input(\"Enter 1 for exp,2 for log, 3 for square transformation, 4 for box cox ,5 for yeo johnson: \"))\n",
        "    if func==1:\n",
        "      train=np.exp(train)\n",
        "    elif func==2:\n",
        "      train=np.log(train)\n",
        "    elif func==3:\n",
        "      train=train**2\n",
        "    elif func==4:\n",
        "      pt=PowerTransformer(method='box-cox',standardize=True)\n",
        "      train=pt.fit_transform(train)\n",
        "    elif func==5:\n",
        "      pt=PowerTransformer(method='yeo-johnson',standardize=True)\n",
        "      train=pt.fit_transform(train)\n",
        "    return train\n",
        "\n",
        "  train_numeric=pd.DataFrame(to_normalize(train3.copy()))\n",
        "  new_train=pd.concat([train_numeric,train4],axis=1)\n",
        "\n",
        "  plot_histograms(train_numeric,train_numeric.columns)\n",
        "\n",
        "  #check for correlation\n",
        "  def correlation_check(train1):\n",
        "    train1.corr()\n",
        "    plt.figure(figsize=(20,10))\n",
        "    print(sns.heatmap(train1.corr()))\n",
        "    print(\"\\n\")\n",
        "  correlation_check(new_train.copy())\n",
        "\n",
        "  #outlier detection using IQR method\n",
        "  #remove less for training\n",
        "  import numpy as np\n",
        "\n",
        "  def outlier_detection_iqr(train):\n",
        "    l=[]\n",
        "    # print(sns.boxplot(x=train[i],whis=1.5))\n",
        "    #check outliers for numeric data\n",
        "    for i in train.columns:\n",
        "      q1=np.percentile(train[i],25,method='midpoint')\n",
        "      q3=np.percentile(train[i],75,method='midpoint')\n",
        "      iqr=q3-q1\n",
        "      lower_lim=q1-1.5*iqr\n",
        "      upper_lim=q3+1.5*iqr\n",
        "      l.append([lower_lim,upper_lim])\n",
        "      # print(lower_lim,upper_lim)\n",
        "      # train_new=train[(train[i]>=lower_lim) & (train[i]<=upper_lim)]\n",
        "      # return train\n",
        "    d={}\n",
        "    k=0\n",
        "    for i in train.columns:\n",
        "      d[i]=l[k]\n",
        "      k=k+1\n",
        "    print('quantile(lower and upper) limits where outliers are not detected for each column:')\n",
        "    print([r for r in d.items()])\n",
        "    print('\\n')\n",
        "    feat=random.choice(new_train.columns)\n",
        "    print(sns.boxplot(x=new_train[feat],whis=1.5))\n",
        "\n",
        "  outlier_detection_iqr(train_numeric.copy())\n",
        "  return new_train\n",
        "\n",
        "# new_train=preprocessing(train)"
      ],
      "metadata": {
        "id": "DCUUKPLsetBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxtY6FCwLRdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=int(input('Enter 1 if you have a separate test dataset, 2 if you have only one and you want to split it: '))\n",
        "ans"
      ],
      "metadata": {
        "id": "8gdtBh-gNaqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ans==1:\n",
        "  train=t.copy()\n",
        "  test=pd.read_csv(input('Enter the path for the test dataset: '))\n",
        "elif ans==2:\n",
        "  lim=int(input('Enter limit where you want to split: '))\n",
        "  train=t.copy().iloc[0:lim]\n",
        "  test=t.copy().iloc[lim:len(t)]\n"
      ],
      "metadata": {
        "id": "V6yP9UUPN3Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train=preprocessing(train)\n",
        "print(new_train)"
      ],
      "metadata": {
        "id": "xb3d1MrvPj4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test=preprocessing(test)\n",
        "print(new_test)"
      ],
      "metadata": {
        "id": "dOa20kYTPsMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetsAnalysis(new_train,new_test):\n",
        "  for i in [new_train,new_test]:\n",
        "    print(i)\n",
        "    print(i.info())\n",
        "    print(i.describe())\n",
        "  sns.set(style=\"whitegrid\")\n",
        "  # new_train.reset_index()\n",
        "  # new_test.reset_index()\n",
        "  comparison = datacompy.Compare(new_train,new_test,join_columns=[('target')],df1_name='train',df2_name='test')\n",
        "  print(comparison.report())\n",
        "  l=[]\n",
        "  l=list(set(new_train)&set(new_test))\n",
        "  print('enter category')\n",
        "  categorical=list(map(str,input().split()))\n",
        "  numeric=[i for i in new_train.columns if i not in categorical]\n",
        "\n",
        "  def plotHistCol(data,col):\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.kdeplot(x=data[col],label='train')\n",
        "    sns.kdeplot(x=data[col],label='test')\n",
        "    plt.legend()\n",
        "\n",
        "  def plotBarAndHist(new_train,new_test,numeric):\n",
        "    feat=random.choice(numeric)\n",
        "    # plt.subplot(1,2,1)\n",
        "    # sns.histplot(x=new_train[feat],label='train',bins=100)\n",
        "    # sns.histplot(x=new_test[feat],label='test',bins=100)\n",
        "    # plt.legend()\n",
        "    plotHistCol(new_train,feat)\n",
        "    plotHistCol(new_test,feat)\n",
        "    plt.figure()\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.kdeplot(x=new_train[feat],label='train')\n",
        "    sns.kdeplot(x=new_test[feat],label='test')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "  print('kde plots\\n')\n",
        "  plotBarAndHist(new_train,new_test,numeric)\n",
        "\n",
        "  def categoricalDist(new_train,new_test,category):\n",
        "    cat=random.choice(category)\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.countplot(x=new_train[cat])\n",
        "    plt.subplot(1,2,2)\n",
        "    sns.countplot(x=new_test[cat])\n",
        "  categoricalDist(new_train,new_test,categorical)\n",
        "\n",
        "  def scatterPlot(new_train,new_test,numeric):\n",
        "    m=min(len(new_train),len(new_test))\n",
        "    r1=random.choice(numeric)\n",
        "    r2=random.choice(numeric)\n",
        "    test1=new_test.iloc[0:m]\n",
        "    train1=new_train.iloc[0:m]\n",
        "    plt.figure()\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.scatter(x=test1[r1],y=test1[r2],c='r')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter(x=train1[r1],y=train1[r2],c='g')\n",
        "    plt.figure()\n",
        "\n",
        "  scatterPlot(new_train,new_test,numeric)\n",
        "\n",
        "  def tTestStatistic(new_train,new_test,cols):\n",
        "    d={}\n",
        "    for feat in cols:\n",
        "      train1=new_train[feat]\n",
        "      test1=new_test[feat]\n",
        "      #Check the means for the population distribution and sample distribution\n",
        "      print('means:',train1.mean(),test1.mean())\n",
        "      #for population distribution - train data\n",
        "      #to check if sample data comes from the train\n",
        "      #conducting a t-test for 95% confidence level\n",
        "      result=stats.ttest_1samp(a=test1,popmean=train1.mean())\n",
        "\n",
        "      # print(f'1 sample t test: {result} ')\n",
        "\n",
        "      #checking quantiles to see if t statistic outside quantile\n",
        "      #reject null hypo if true\n",
        "      # for two tailed t test\n",
        "      q1=stats.t.ppf(q=0.025,df=len(new_test)-1)\n",
        "      q2=stats.t.ppf(q=0.975,df=len(new_test)-1)\n",
        "\n",
        "      # print(f'quantiles : {q1} {q2}')\n",
        "\n",
        "      #calculating p value\n",
        "      t_value=result.statistic\n",
        "      # For a two-tail test, the two critical values are the 2.5th and the 97.5th percentiles of the t-distribution with nâˆ’1 degrees of freedom;\n",
        "      #  reject the null in favor of the alternative if the t-statistic is less than the 2.5th percentile or greater than the 97.5th percentile.\n",
        "\n",
        "      if q1<t_value<q2:\n",
        "        # print(f\"t-statistic: {t_value}\\n\")\n",
        "        #print t value and their column falling in the non rejection region,use dictionary\n",
        "        d[feat]=t_value\n",
        "    print([i for i in d.items()])\n",
        "    print('no of cols in above:',len(d))\n",
        "\n",
        "  print('columns which doesnt hold much difference in the mean of population distribution')\n",
        "  print('printing the no of columns as well')\n",
        "  tTestStatistic(new_train,new_test,new_train.columns)\n",
        "\n",
        "  #Kolmogorov-Smirnov test-non parametric test to determine two samples come from the same distribution\n",
        "  #if pvalue>alpha ->cannot reject null hypo,else reject null hypo\n",
        "  def KsStatistics(new_train,new_test):\n",
        "    d={}\n",
        "    m=min(len(new_train),len(new_test))\n",
        "    for i in new_train.columns :\n",
        "      statistic,pvalue=ks_2samp(new_train[0:m][i],new_test[0:m][i])\n",
        "      # print(f'test statistic {statistic}  p-value :{pvalue}')\n",
        "      alpha=0.05\n",
        "      if pvalue>alpha:\n",
        "        d[i]=pvalue\n",
        "    print([i for i in d.items()])\n",
        "    print('no of cols in above:',len(d))\n",
        "  print('printing cols which determine that the two samples come from the same distribution and also the number of cols')\n",
        "  KsStatistics(new_train,new_test)\n",
        "\n",
        "  def addGaussianNoise(dataset):\n",
        "    #add random noise to the preprocessed data\n",
        "    noise_factor=0.5\n",
        "    noisyData=dataset+noise_factor*np.random.normal(loc=0.0,scale=1.0,size=dataset.shape)\n",
        "    # print(noisyData)\n",
        "    return pd.DataFrame(noisyData)\n",
        "\n",
        "\n",
        "\n",
        "  #Using denoising autoencoders\n",
        "  def denoisingAutoencoding(dataset,noisyData):\n",
        "    input_layer = Input(shape=(dataset.shape[1],))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    decoded = Dense(dataset.shape[1], activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "    #compiling and training the autoencoder\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    autoencoder.fit(noisyData,dataset, epochs=100, batch_size=32, shuffle=True)\n",
        "    return autoencoder\n",
        "\n",
        "  # denoised_data = autoencoder.predict(noisyData)\n",
        "  # return pd.DataFrame(denoised_data)\n",
        "\n",
        "  noisyData=addGaussianNoise(new_train.copy())\n",
        "  autoEncoder=denoisingAutoencoding(new_train,noisyData)\n",
        "\n",
        "  denTrain=pd.DataFrame(autoEncoder.predict(noisyData))\n",
        "  denTest=pd.DataFrame(autoEncoder.predict(new_test))\n",
        "  print(denTrain)\n",
        "  print('identify the categorical column from above data and enter column names:')\n",
        "  cats=list(map(int,input().split()))\n",
        "  nums=[x for x in denTrain.columns if x not in cats]\n",
        "  denTrain[cats]=denTrain[cats].apply(lambda x:round(x))\n",
        "  denTest[cats]=denTest[cats].apply(lambda x:round(x))\n",
        "  print('DENOISED TRAIN DATA:',denTrain)\n",
        "  plotBarAndHist(denTrain,denTest,nums)\n",
        "  KsStatistics(denTrain,denTest)\n",
        "  #to check are we getting similar feature importance between the two datasets?\n",
        "  def featureSelection(dataset,nf):\n",
        "    print('the dataset is:',dataset)\n",
        "    #figure out the dataset and divide into X and y\n",
        "    X=dataset.drop('target',axis=1)\n",
        "    y=dataset['target']\n",
        "    # X_train,y_train,X_test,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n",
        "    rf=RandomForestClassifier()\n",
        "    rf.fit(X,y)\n",
        "    importances=rf.feature_importances_\n",
        "    final_train=pd.DataFrame({\"Features\":X.columns,\"Importances\":importances})\n",
        "    final_train.set_index('Importances')\n",
        "\n",
        "    #sort for good visualization\n",
        "    final_train=final_train.sort_values('Importances')\n",
        "\n",
        "    #plot the feature importance in bars\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.xticks(rotation=45)\n",
        "    sns.barplot(x=\"Features\",y=\"Importances\",data=final_train)\n",
        "\n",
        "    #selecting features with high importance\n",
        "    final_train_new=final_train.copy()\n",
        "    final_train_new.sort_values('Importances',ascending=False)\n",
        "    l=list(final_train_new.head(nf)[\"Features\"])\n",
        "    return l\n",
        "\n",
        "  nf=int(input('enter limit  feature imp:-'))\n",
        "\n",
        "  l1=featureSelection(new_train,nf)\n",
        "  print('\\n')\n",
        "  l2=featureSelection(new_test,nf)\n",
        "  print('train dataset feature importance:-',l1)\n",
        "  print('train dataset feature importance:-',l2)\n",
        "  # def roundingCategorical(dataset,cats):\n",
        "  #   den_train1=dataset.copy()\n",
        "  #   den_train1[cats]=den_train[cats].apply(lambda x:round(x))\n",
        "  # return\n",
        "  print(f\"common importance features among the two datasets :{list(set(l1) and set(l2))}\")\n",
        "\n",
        "DatasetsAnalysis(new_train,new_test)"
      ],
      "metadata": {
        "id": "jpIfpYQCQFoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "q5pqreRdlwtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "SsTWLXKuDdGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "#  !pip install ruptures\n",
        "# download it in the file youre importing"
      ],
      "metadata": {
        "id": "CP0pFdudGanP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ruptures as rpt"
      ],
      "metadata": {
        "id": "LWCDvJujGf1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxG2G_5OxoB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processRegimesIdentification(new_train,new_test):\n",
        "  new_train1=new_train.copy()\n",
        "  new_test1=new_test.copy()\n",
        "  # store and remove the target\n",
        "  output1=new_train1['target']\n",
        "  output2=new_test1['target']\n",
        "  new_train1.drop('target',axis=1,inplace=True)\n",
        "  new_test1.drop('target',axis=1,inplace=True)\n",
        "  # plot a random column from your data\n",
        "  def analyzeDataFeatures(new_train1):\n",
        "    for i in range(2):\n",
        "      plt.figure(figsize=(12,8))\n",
        "      r=random.choice(new_train1.columns)\n",
        "\n",
        "      plt.plot(new_train1.index,new_train1[r],color='g')\n",
        "      plt.xlabel('Timestamp')\n",
        "      plt.ylabel(f\"Feature {r}\")\n",
        "\n",
        "  analyzeDataFeatures(new_train1)\n",
        "\n",
        "\n",
        "\n",
        "  #Use changepoint analysis to separate out the control regimes\n",
        "  def changePointDetection_1feature(new_train1):\n",
        "    # l=[]\n",
        "    d={}\n",
        "    count=0\n",
        "    for col in new_train1.columns:\n",
        "      algo=rpt.Pelt(model='rbf').fit(new_train1[col].values)\n",
        "      result=algo.predict(pen=10)\n",
        "      # l.append(result)\n",
        "      d[col]=result\n",
        "      if count==0:\n",
        "        print('Visualizing the changepoints for the first column')\n",
        "        fig,ax=plt.subplots()\n",
        "        ax.plot(new_train1[col],color='tab:red')\n",
        "        for res in result:\n",
        "          ax.axvline(x=res,color='k',linestyle='--')\n",
        "        count=count+1\n",
        "    # print(l)\n",
        "    l1=[]\n",
        "    threshold=int(input('Enter threshold to separate out the control regimes: '))\n",
        "    for key in d.keys():\n",
        "      if len(d[key])==threshold:\n",
        "        l1.append(key)\n",
        "    # print(l1)\n",
        "    return l1\n",
        "\n",
        "  controlRegimes1=changePointDetection_1feature(new_train1)\n",
        "\n",
        "  def changePointDetection_2features(new_train1):\n",
        "      print('FOR 2 FEATURES')\n",
        "      r1 = random.choice(new_train1.columns)\n",
        "      while True:\n",
        "          r2 = random.choice(new_train1.columns)\n",
        "          if r2 != r1:\n",
        "              break\n",
        "      print(f'Features are {r1} and {r2}')\n",
        "      l=[]\n",
        "      a = np.vstack([new_train1[r1].values, new_train1[r2].values])\n",
        "      algo = rpt.Pelt(model='rbf').fit(a)\n",
        "      result = algo.predict(pen=50)\n",
        "      print(result)\n",
        "      fig,ax=plt.subplots()\n",
        "      ax.plot(new_train1[r1],color='orange')\n",
        "      ax.plot(new_train1[r2],color='green')\n",
        "      for res in result:\n",
        "        ax.axvline(x=res,color='k',linestyle='--')\n",
        "      if len(result)>0:\n",
        "        l.append([r1,r2])\n",
        "      return l\n",
        "  print('CHANGEPOINT DETECTION ANALYSIS FOR 2 FEATURES')\n",
        "  l=changePointDetection_2features(new_train1)\n",
        "  controlRegimes=controlRegimes1\n",
        "\n",
        "  print(controlRegimes1)\n",
        "  print('No of control regimes based on a single feature: ',len(controlRegimes1))\n",
        "  # controlRegimes1.append(l)\n",
        "  # controlRegimes=[x for j in set(controlRegimes1)]\n",
        "  conditionRegimes=list(input('Enter the condition features: ').split())\n",
        "  processData=[x for x in new_train1.columns if x not in controlRegimes and x not in conditionRegimes]\n",
        "\n",
        "  def clusteringAnalysis(X_train1,X_test1):\n",
        "     def dimensionReduction(X_train1,X_test1):\n",
        "      print('Reducing to 2 dimensions')\n",
        "      pca=PCA(n_components=2)\n",
        "      X_train=pca.fit_transform(X_train1)\n",
        "      X_test=pca.transform(X_test1)\n",
        "      return X_train,X_test\n",
        "\n",
        "     X_train1,X_test1=dimensionReduction(X_train1,X_test1)\n",
        "\n",
        "     def elbowMethodKmeans(X_train1):\n",
        "      mod = KMeans(random_state=1)\n",
        "      plt.figure()\n",
        "      no = int(input('Enter 1 for silhouette score or 2 for calinski harabasz: '))\n",
        "      d = {1: 'silhouette', 2: 'calinski_harabasz'}\n",
        "      visualizer = KElbowVisualizer(mod, k=(2, 15), metric=d[no], timings=True)\n",
        "      visualizer.fit(X_train1)\n",
        "      visualizer.show()\n",
        "\n",
        "     elbowMethodKmeans(X_train1)\n",
        "\n",
        "     def visualizeClusters(labels,X):\n",
        "      plt.figure()\n",
        "      uLabels=np.unique(labels)\n",
        "      # X['LABS']=labels\n",
        "      for i in uLabels:\n",
        "        indices = np.where(labels == i)[0] #get the indices\n",
        "        print(f'LABEL {i}: {indices}')\n",
        "        plt.scatter(X[labels == i , 0] , X[labels == i , 1] , label = i)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "     def KMeanClustering(X_train1):\n",
        "      i = int(input('Enter the number of clusters: '))\n",
        "      kmeans = KMeans(init='random', n_clusters=i, n_init=10, max_iter=300, random_state=42)\n",
        "      kmeans.fit(X_train1)\n",
        "      # Get the labels\n",
        "      labels = kmeans.labels_\n",
        "      print(f'Labels: {labels}')\n",
        "      visualizeClusters(labels,X_train1)\n",
        "     print('K means for train data')\n",
        "     KMeanClustering(X_train1)\n",
        "     print('K means for test data')\n",
        "     KMeanClustering(X_test1)\n",
        "  if len(controlRegimes)==0:\n",
        "    print('NO CONTROL REGIMES PRESENT!')\n",
        "    pass\n",
        "  else:\n",
        "    clusteringAnalysis(new_train1[controlRegimes],new_test1[controlRegimes])\n",
        "\n",
        "  clusteringAnalysis(new_train1[conditionRegimes],new_test1[conditionRegimes])\n",
        "  clusteringAnalysis(new_train1[processData],new_test1[processData])\n",
        "  print('Clustering on the whole dataset after reducing it to 2 dimensions!')\n",
        "  clusteringAnalysis(new_train1,new_test1)\n",
        "processRegimesIdentification(new_train,new_test)"
      ],
      "metadata": {
        "id": "YaotsVzaD4qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEawP2_kUOMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_dZxPvq6VI4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfgXNMp2QkQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}